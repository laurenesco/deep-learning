{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "302b2678-3eab-44ec-8b51-7ae3fa07014c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributions as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfe29c94-50be-4283-af2b-c25c1cf2bbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([0.])\n",
      "tensor([1.])\n",
      "tensor([0.])\n",
      "tensor([1.])\n",
      "tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "## Lets create a Beroulli distribution\n",
    "bernoulli = dist.Bernoulli(torch.tensor([.5]))\n",
    "bernoulli\n",
    "\n",
    "## Sample the distribution\n",
    "for _ in range(10):\n",
    "    print(bernoulli.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04341102-6505-4e93-a9df-67908a7ade65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.4950000047683716\n",
      "Variance: 0.2502252161502838\n"
     ]
    }
   ],
   "source": [
    "## Calculate some description statistics\n",
    "samples = [bernoulli.sample() for _ in range(1000)]\n",
    "print(f\"Mean: {torch.mean(torch.stack(samples))}\")\n",
    "print(f\"Variance: {torch.var(torch.stack(samples))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc8f158d-c5e8-46ce-b853-404af2b5d2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.1049])\n",
      "tensor([-0.1747])\n",
      "tensor([0.8088])\n",
      "tensor([0.4183])\n",
      "tensor([-0.3044])\n",
      "tensor([0.7036])\n",
      "tensor([-1.9048])\n",
      "tensor([-0.8897])\n",
      "tensor([-1.0097])\n",
      "tensor([-0.7746])\n"
     ]
    }
   ],
   "source": [
    "## Lets create a normal distribution\n",
    "gaussian = dist.Normal(torch.tensor([0.0]), torch.tensor([1.0]))\n",
    "for _ in range(10):\n",
    "    print(gaussian.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49af122-55cf-4397-a026-8d7cc2c0eda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.1114])\n",
      "tensor([1.1891])\n",
      "tensor([-0.2296])\n",
      "tensor([-1.3138])\n",
      "tensor([-2.2008])\n",
      "tensor([0.0279])\n",
      "tensor([0.6364])\n",
      "tensor([1.9385])\n",
      "tensor([-0.1980])\n",
      "tensor([1.3477])\n",
      "Mean: -0.04635487496852875\n",
      "Variance: 0.9731736183166504\n"
     ]
    }
   ],
   "source": [
    "samples = [gaussian.sample() for _ in range(1000)]\n",
    "print(f\"Mean: {torch.mean(torch.stack(samples))}\")\n",
    "print(f\"Variance: {torch.var(torch.stack(samples))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e546434-269a-4d75-8825-b4c29aba7048",
   "metadata": {},
   "source": [
    "### Key Takeaway\n",
    "In sampling any probability distribution, you can think of the PyTorch function that generates the distributions as the **data generating distribution**. It is the underlying distribution that is pure and beautiful (think nature), and we only get to observe samples from it. Since we only get to observe samples from it, **the samples never fully match the original distribution**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep-learning]",
   "language": "python",
   "name": "conda-env-deep-learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
