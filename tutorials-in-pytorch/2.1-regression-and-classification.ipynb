{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb74566a-c597-418b-9804-994d524cc3ad",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "<img src=\"./images/linear-regression.jpeg\" width=\"400\" style=\"display: block; margin: auto;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2db0a5e9-98bf-4e91-a45a-237ef978d194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1f5b864-dab0-461e-87ee-24ef3473574e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.2479,  0.2705, -0.0212, -0.1447,  0.1133,  0.2561, -0.2148, -0.0716,\n",
      "          0.1681, -0.0261]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0878], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Create a linear regression model\n",
    "# Arguments: # of inputs, # of outputs\n",
    "\n",
    "# Note: Each time you reload the model you will get slightly different results,\n",
    "#       since the model is initialized with different weights. This is important.\n",
    "\n",
    "model = nn.Linear(10, 1) \n",
    "\n",
    "# Inspect the model \n",
    "print(model.weight)\n",
    "print(model.bias) # Only one bias since only one output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7a21ac2-0ab4-4ec7-a649-00e99145154e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0061], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(10)\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e81c43-4a35-4c83-b31f-ca8460905a21",
   "metadata": {},
   "source": [
    "**Limitations:** Linear models are ***not*** good for cyclic functions or quadratic functions. They cannot do anything that really is not a nice straight line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8eea2f-e211-40d3-9774-674e3a0eb69d",
   "metadata": {},
   "source": [
    "## Linear Binary Classification\n",
    "\n",
    "What if, instead, we want to classify our inputs based on if they are in class 1 or class 2? In this case, we regress to probabilities of belonging to class 1 vs class 2. The sigmoid function takes values ranging from $-\\infty$ to $\\infty$ and squash them so that they are between 0 and 1.\n",
    "\n",
    "<img src=\"./images/binary-classification.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "Binary classification model: \n",
    "\n",
    "$$ f_{\\theta} : \\mathbb{R}^n \\implies [0,1] $$\n",
    "\n",
    "Linear binary classification:\n",
    "\n",
    "$$f_{\\theta}(\\mathbf{x}) = \\sigma(\\mathbf{Wx + b}) $$\n",
    "\n",
    "$$ \\sigma(x) = \\frac{1}{1+e^{-x}} $$\n",
    "\n",
    "Parameters: \n",
    "\n",
    "$$ \\theta = (\\mathbf{W,b}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ba5119c-4a9f-4033-9523-7a8b20a38865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearClassifier(\n",
      "  (fc): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n",
      "Parameter containing:\n",
      "tensor([[ 0.0271,  0.2877, -0.1170,  0.0942,  0.1914, -0.1088,  0.0101,  0.2968,\n",
      "          0.2176,  0.0385]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1320], requires_grad=True)\n",
      "\n",
      "Testing with tensor of 0's: tensor([0.4670], grad_fn=<SigmoidBackward0>)\n",
      "Testing with tensor of 1's: tensor([0.6912], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# A couple special rules for using torch.nn.Module:\n",
    "#  1. Always immediately call super().__init__()\n",
    "#  2. Never put a sigmoid directly into the model\n",
    "#     (We do it here just to show what it would look like)\n",
    "\n",
    "class LinearClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(input_dim, output_dim) # Defining a linear regressin layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.functional.sigmoid(self.fc(x)) # Also pass the data through a sigmoid function\n",
    "\n",
    "model = LinearClassifier(10,1)\n",
    "print(model)\n",
    "print(model.fc.weight)\n",
    "print(model.fc.bias)\n",
    "\n",
    "x = torch.zeros(10)\n",
    "print(f\"\\nTesting with tensor of 0's: {model(x)}\")\n",
    "\n",
    "x = torch.ones(10)\n",
    "print(f\"Testing with tensor of 1's: {model(x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0260d593-4f53-4ffd-b163-d79bbbde40bd",
   "metadata": {},
   "source": [
    "**Limitations:** Linear classifiers are not good at dealing with non-linear decision boundaries. They can also not express a function that has two separating planes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05e1351-b0d1-4303-99e8-a77b4dcdafad",
   "metadata": {},
   "source": [
    "## Linear Multi-Class Classification\n",
    "What if we have $c$ different classes that we want to separate, or classify? In this case again, we regress to probabilities. The softmax function does this for multiple classes (whereas sigmoid does it for binary problems)\n",
    "\n",
    "Multi-class classification model:\n",
    "\n",
    "$$ f_\\theta : \\mathbb{R}^n \\rightarrow \\mathbb{P}^c \\;\\;\\; where \\;\\;\\; \\mathbb{p}^c \\in \\mathbb{R}^c_+ \\;\\;\\;\\; \\forall_{\\mathbf{y}\\in\\mathbb{P}^c}\\mathbf{1}^\\top \\mathbf{y} = 1 $$\n",
    "\n",
    "Linear multi-class classification:\n",
    "\n",
    "$$ f_\\theta (\\mathbf{x}) = softmax(\\mathbf{Wx+b}) $$\n",
    "$$ softmax(\\mathbf{v}_i = \\frac{e^{v_i}}{\\sum^n_{j=1}e^{v_j}}) $$\n",
    "\n",
    "Parameters:\n",
    "\n",
    "$$ \\theta = (\\mathbf{W,b) $$\n",
    "\n",
    "## Softmax function\n",
    "\n",
    "The softmax function takes a vector of d real valued numbers from $-\\infty$ to $+\\infty$, and first exponentiates them which maps them from $0$ to $+\\infty$. Then, it normalizes them to all sum up to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5657cdfd-c128-4da5-bba1-070eb4d67750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearClassifier(\n",
      "  (fc): Linear(in_features=10, out_features=4, bias=True)\n",
      ")\n",
      "Parameter containing:\n",
      "tensor([[ 0.1521,  0.1082, -0.1903,  0.0762,  0.0773, -0.1393,  0.2910, -0.1023,\n",
      "         -0.1709,  0.0615],\n",
      "        [ 0.2166,  0.0900, -0.2946, -0.2571, -0.2505,  0.1324,  0.2025,  0.0415,\n",
      "         -0.0054, -0.1827],\n",
      "        [ 0.2858,  0.1660, -0.1890,  0.2525, -0.1384, -0.3098, -0.2123,  0.0535,\n",
      "          0.2263,  0.2635],\n",
      "        [-0.2173,  0.3146,  0.0960, -0.1536,  0.0641, -0.0596,  0.3121, -0.0352,\n",
      "         -0.2341, -0.0897]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0121, -0.0879, -0.0439,  0.0899], requires_grad=True)\n",
      "\n",
      "Testing with tensor of 0's: tensor([0.2498, 0.2316, 0.2420, 0.2766], grad_fn=<SoftmaxBackward0>)\n",
      "Testing with tensor of 1's: tensor([0.2672, 0.1547, 0.3274, 0.2507], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class LinearClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_dim, n_classes):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(input_dim, n_classes) # Defining a linear regressin layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.functional.softmax(self.fc(x), dim=-1) # Also pass the data through a sigmoid function\n",
    "\n",
    "model = LinearClassifier(10,4)\n",
    "print(model)\n",
    "print(model.fc.weight)\n",
    "print(model.fc.bias)\n",
    "\n",
    "x = torch.zeros(10)\n",
    "print(f\"\\nTesting with tensor of 0's: {model(x)}\")\n",
    "\n",
    "x = torch.ones(10)\n",
    "print(f\"Testing with tensor of 1's: {model(x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988dda0a-dba8-472f-b159-ae02caefe305",
   "metadata": {},
   "source": [
    "Note that you can technically still put the sigmoid function in for multiclass classification, which yields a multiple binary classifier. When would you want to use each:\n",
    "\n",
    "Multiclass classifier (softmax):\n",
    "- Describes exactly one category\n",
    "- No negative examples\n",
    "- Calibrated probabilities\n",
    "- Used for mutually exclusive categories\n",
    "\n",
    "Examples:\n",
    "- Predicting the weather\n",
    "- Predicting the name of an animal\n",
    "- Predicting the next word in a sentence\n",
    "\n",
    "Multiple binary classifier (sigmoid):\n",
    "- Allows for multiple categories\n",
    "- Requires negative examples\n",
    "- Uncalibrated probabilities\n",
    "- Used for multi-label tagging\n",
    "\n",
    "Examples:\n",
    "- Predicting where in Texas it will rain\n",
    "- Predicting attributes of an animal\n",
    "- Predicting which books a sentence can be found in"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
