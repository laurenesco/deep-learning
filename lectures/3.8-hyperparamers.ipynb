{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e2b59f4-66e1-433a-8403-98a96687cdc7",
   "metadata": {},
   "source": [
    "# Lecture Notes: Hyperparameters and the Limits of Gradient Descent\n",
    "\n",
    "## Recap\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/381.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "We've previously studied **stochastic gradient descent (SGD)** and how it is used to optimize model parameters during training.\n",
    "\n",
    "## Can SGD Optimize Everything?\n",
    "\n",
    "No — while SGD can optimize any parameter we can compute gradients with respect to, **some important parameters in neural network training are *not* differentiable**.\n",
    "\n",
    "### Example:\n",
    "- Epsilon ($\\epsilon$), in the update rule:\n",
    "  \n",
    "  $$ \\theta \\leftarrow \\theta - \\epsilon \\cdot \\nabla_\\theta L(\\theta) $$\n",
    "\n",
    "  The learning rate $\\epsilon$ is **not** optimized via SGD — it's set manually.\n",
    "\n",
    "---\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/382.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "### Definition:\n",
    "**Hyperparameters** are parameters that **cannot be learned** directly through gradient descent.\n",
    "\n",
    "They are **external to the training loop** and need to be **manually specified or tuned**.\n",
    "\n",
    "### Common hyperparameters include:\n",
    "- Learning rate ($\\epsilon$)\n",
    "- Number of epochs\n",
    "- Model architecture (e.g., number of layers, hidden units)\n",
    "- Loss function\n",
    "- Choice of optimizer or optimization variant (e.g., SGD, Adam, RMSprop)\n",
    "\n",
    "These are **not part of the model's differentiable graph**, so gradient-based optimization can’t adjust them.\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/383.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "## How Are Hyperparameters Tuned?\n",
    "\n",
    "There is **no universal algorithm** that reliably finds the best hyperparameter configuration.\n",
    "\n",
    "### In practice:\n",
    "- Hyperparameters are often **tuned by hand**\n",
    "- Choosing good values often relies on:\n",
    "  - Experience\n",
    "  - Intuition\n",
    "  - Trial and error\n",
    "  - Iterating over what worked for similar tasks\n",
    "\n",
    "This manual tuning process is humorously referred to as:\n",
    "\n",
    "> \"**Graduate Student Descent**\" — because graduate students spend a lot of time adjusting hyperparameters manually during research.\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/384.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- **SGD can only optimize differentiable parameters**.\n",
    "- **Hyperparameters are non-learnable** and require manual or external tuning.\n",
    "- These include settings related to:\n",
    "  - The training algorithm\n",
    "  - The model architecture\n",
    "  - The loss function\n",
    "- Hyperparameter tuning is **iterative** and often guided by experimentation and domain experience.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
