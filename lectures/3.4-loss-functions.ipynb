{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e248992b-9c13-494b-82f2-9bdfefa5c802",
   "metadata": {},
   "source": [
    "# Loss Functions and Output Transformations in Deep Networks\n",
    "\n",
    "## Overview\n",
    "- Deep networks can approximate any continuous function from $\\mathbb{R}^n \\rightarrow \\mathbb{R}^m$\n",
    "- However, many tasks need non-continuous outputs (e.g., classes, positive values)\n",
    "- We use:\n",
    "  - **Output transformations** (for inference)\n",
    "  - **Loss functions** (for training)\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/ortldr.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "## Output Transformations (for Inference)\n",
    "- **Each output transformation has its own very specific loss function**\n",
    "- Applied **after** the model is trained\n",
    "- Convert raw network outputs into usable values\n",
    "- Examples:\n",
    "  - Ensure positive outputs using ReLU or Softplus\n",
    "  - Map outputs to class probabilities using sigmoid or softmax\n",
    "\n",
    "## Loss Functions (for Training)\n",
    "- Work with raw outputs (logits)\n",
    "- Must **always** provide **useful gradients** to guide learning\n",
    "- Represented as:\n",
    "  - Lowercase $l$: loss for individual data points\n",
    "  - Uppercase $L$: total loss over dataset\n",
    "\n",
    "---\n",
    "\n",
    "## Goal of Training\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/loss.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "- Minimize **expected loss**:\n",
    "  $$\n",
    "  \\mathbb{E}_{(x, y) \\sim \\mathcal{D}}[l(f_\\theta(x), y)]\n",
    "  $$\n",
    "- Good loss functions:\n",
    "  - Provide large gradients when the model is wrong\n",
    "  - Provide small or zero gradients when the model is right\n",
    "\n",
    "---\n",
    "\n",
    "# **Types of Losses**\n",
    "\n",
    "## Regression Losses\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/regrl.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "### L1 Loss (Absolute Error)\n",
    "- $l = |f_\\theta(x) - y|$\n",
    "- Also called \"Manhattan\" or \"taxi\" distance\n",
    "\n",
    "### L2 Loss (Squared Error)\n",
    "- $l = (f_\\theta(x) - y)^2$\n",
    "- Also called \"Euclidean distance\"\n",
    "\n",
    "**Note**: In practice, L1 vs L2 doesn’t matter much — both perform well.\n",
    "\n",
    "---\n",
    "\n",
    "## Binary Classification\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/closs.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "- Labels: $y \\in \\{0, 1\\}$\n",
    "- Model outputs a logit → pass through sigmoid:\n",
    "  $$\n",
    "  \\sigma(o) = \\frac{1}{1 + e^{-o}}\n",
    "  $$\n",
    "- Then compute the **negative log likelihood**:\n",
    "  $$\n",
    "  l = -[y \\cdot \\log(\\sigma(o)) + (1 - y) \\cdot \\log(1 - \\sigma(o))]\n",
    "  $$\n",
    "- This is also known as **binary cross-entropy loss**\n",
    "\n",
    "### Why use the log?\n",
    "- Without the log:\n",
    "  - Gradient is **flat** when the model is very wrong → no learning\n",
    "- With log:\n",
    "  - Gradient is **large** when wrong → faster learning\n",
    "  - Gradient is **small** when right → stable convergence\n",
    "\n",
    "### Numerical Stability\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/binl.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "- Very wrong predictions (large negative logits) can make $\\sigma(o) \\to 0$\n",
    "- Then $\\log(\\sigma(o))$ becomes undefined (NaN)\n",
    "- Solution: use PyTorch's built-in **`BCEWithLogitsLoss`**, which combines sigmoid + log safely\n",
    "\n",
    "---\n",
    "\n",
    "## Multi-Class Classification\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/mcc.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "- Labels: $y \\in \\{1, 2, ..., C\\}$\n",
    "- Output: vector of logits → pass through softmax:\n",
    "  $$\n",
    "  \\text{softmax}(o_i) = \\frac{e^{o_i}}{\\sum_j e^{o_j}}\n",
    "  $$\n",
    "- Then use **negative log-likelihood** of the correct class\n",
    "\n",
    "### Cross-Entropy Loss\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/mccl.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "- PyTorch function: `CrossEntropyLoss`\n",
    "- Takes:\n",
    "  - Raw logits from the model (not softmaxed)\n",
    "  - Ground truth labels (as class indices)\n",
    "- Internally applies `log(softmax(logits))` in a numerically stable way\n",
    "\n",
    "### Softmax issues\n",
    "- If logits differ by large amounts (e.g., >100), softmax can output exact zeros\n",
    "- Taking log of zero → NaN\n",
    "- Solution: use `CrossEntropyLoss`, not manual softmax + log\n",
    "\n",
    "---\n",
    "\n",
    "## Summary Table\n",
    "\n",
    "| Task                     | Output Type   | Recommended Loss Function              |\n",
    "|--------------------------|---------------|----------------------------------------|\n",
    "| Regression               | Real values   | L1 or L2 loss                          |\n",
    "| Binary Classification    | 0 or 1        | `BCEWithLogitsLoss` (binary cross-entropy) |\n",
    "| Multi-Class Classification | 1 to C       | `CrossEntropyLoss`                     |\n",
    "\n",
    "> Always use built-in PyTorch loss functions for classification tasks.  \n",
    "> You may hand-code L1 or L2, but avoid manual sigmoid/softmax + log.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
