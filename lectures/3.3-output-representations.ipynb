{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee6c98e8-83b8-4423-a695-ccd6f60b86bb",
   "metadata": {},
   "source": [
    "# Output Representations and Transformations in Deep Networks\n",
    "\n",
    "## Motivation\n",
    "- Deep networks can approximate any continuous function from $ \\mathbb{R}^n \\rightarrow \\mathbb{R}^m $\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/recap.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "- However, tasks often require outputs that are **not continuous**, such as:\n",
    "  - Classification labels\n",
    "  - Probabilities\n",
    "  - Positive-only values\n",
    "- To handle these cases, we apply **output transformations** to the network output\n",
    "\n",
    "---\n",
    "\n",
    "## Notation\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/outputs.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "- Input: $ \\mathbf{x} \\in \\mathbb{R}^n $\n",
    "- Output (deep network function): $ f_\\theta(\\mathbf{x}) $\n",
    "- Deep network: $ f_\\theta$\n",
    "- Output transformations: $g$\n",
    "- Overall model: $ \\psi(\\mathbf{x}) = f_\\theta(\\mathbf{x}) \\circ g $, where $ g $ is the output transformation\n",
    "\n",
    "---\n",
    "\n",
    "## Case 1: Regression\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/posr.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "### Standard Regression\n",
    "- $ \\psi  : \\mathbb{R}^n \\rightarrow \\mathbb{R}$\n",
    "- Output is a real number\n",
    "- No transformation needed:\n",
    "  $$\n",
    "  \\psi(\\mathbf{x}) = f_\\theta(\\mathbf{x})\n",
    "  $$\n",
    "\n",
    "### Positive-only Regression\n",
    "- $ \\psi  : \\mathbb{R}^n \\rightarrow \\mathbb{R}_+$\n",
    "- Goal: output values in $ \\mathbb{R}^+ $\n",
    "- Possible transformations:\n",
    "  1. **ReLU**: $ T(o) = \\max(0, o) $\n",
    "  2. **Softplus**: $ T(o) = \\log(1 + e^o) $\n",
    "     - Always > 0\n",
    "     - Differentiable everywhere\n",
    "\n",
    "#### Trade-offs:\n",
    "- ReLU allows exact zero but has zero gradient when $ o < 0 $\n",
    "- Softplus is smooth and always gives gradient, but never exactly zero\n",
    "\n",
    "---\n",
    "\n",
    "## Case 2: Binary Classification\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/binc.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "- Goal: $ \\psi : \\mathbb{R}^n \\rightarrow [0, 1] $\n",
    "\n",
    "### Transformations:\n",
    "1. **Thresholding** (non-differentiable):\n",
    "   $$\n",
    "   \\hat{y} =\n",
    "   \\begin{cases}\n",
    "   1 & \\text{if } o > 0 \\\\\n",
    "   0 & \\text{otherwise}\n",
    "   \\end{cases}\n",
    "   $$\n",
    "\n",
    "2. **Sigmoid** (differentiable):\n",
    "   $$\n",
    "   \\hat{y} = \\sigma(o) = \\frac{1}{1 + e^{-o}}\n",
    "   $$\n",
    "\n",
    "#### Trade-offs:\n",
    "- Thresholding is simple but not differentiable (no gradients)\n",
    "- Sigmoid is differentiable and gives a **probabilistic interpretation**\n",
    "\n",
    "---\n",
    "\n",
    "## Case 3: Multi-Class Classification\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/genc.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "- Goal: $ \\psi : \\mathbb{R}^n \\rightarrow [1, 2, \\dots, C] $\n",
    "- Network outputs a **C-dimensional vector**\n",
    "\n",
    "### Options:\n",
    "1. **Argmax**:\n",
    "   - Output: index of the highest logit\n",
    "   - Not differentiable\n",
    "   - Ties are broken arbitrarily\n",
    "\n",
    "2. **One-hot encoding**:\n",
    "    $$\\hat{y} = 1 \\; \\text{if} \\; \\mathbb{o}_i \\ge \\mathbb{o}_j \\; \\forall j$$\n",
    "   - Vector with 1 at max index, 0 elsewhere\n",
    "   - Also non-differentiable\n",
    "   - Can indicate ties if multiple 1s present\n",
    "\n",
    "4. **Softmax** (differentiable):\n",
    "   $$\n",
    "   g(\\mathbf{o})_i = \\frac{e^{o_i}}{\\sum_{j=1}^{C} e^{o_j}}\n",
    "   $$\n",
    "   - Converts logits into a probability distribution\n",
    "   - Preferred if gradient flow is required\n",
    "\n",
    "---\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/ortldr.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Do not embed output transformations inside the model**\n",
    "  - Keep model as raw layers: linear + non-linear operations\n",
    "  - Output raw **logits** (for classification) or raw **values** (for regression)\n",
    "  - Apply transformations only during:\n",
    "    - Inference\n",
    "    - Loss calculation (matched to task)\n",
    "\n",
    "### Terminology:\n",
    "- **Logits**: raw network outputs before softmax/sigmoid\n",
    "- **Inference**: Apply transformations like argmax or softmax\n",
    "- **Training**: Use appropriate **loss functions** that internally apply the correct transformations\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "- Deep networks output real values, but many tasks require specific output forms\n",
    "- Output transformations allow us to map from $ \\mathbb{R} $ to:\n",
    "  - $ \\mathbb{R}^+ $\n",
    "  - Binary labels\n",
    "  - Multi-class probabilities or labels\n",
    "- Choose transformations based on:\n",
    "  - Whether you need gradients\n",
    "  - Whether you care about exact 0/1 vs probabilities\n",
    "  - Whether you're training or doing inference\n",
    "- **Always apply transformations outside the model**, not embedded within\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
