{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99b66dc5-8cde-436d-9ea0-25e9b784164e",
   "metadata": {},
   "source": [
    "# Lecture Notes: Why Very Deep Networks Fail to Train\n",
    "\n",
    "## Motivation\n",
    "\n",
    "What happens if we try to train a **very deep network**, like one with 100 layers?\n",
    "\n",
    "- Answer: **It won’t train**\n",
    "- In this section, we explore:\n",
    "  - Why deep networks struggle to train\n",
    "  - How to analyze the problem using simple models\n",
    "  - Techniques to **enable training** very deep networks\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Case Study: A Deep Linear Network\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/411.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "To simplify, consider:\n",
    "- A network of $n$ **linear layers**\n",
    "- Each layer has:\n",
    "  - A **single scalar weight** $w$\n",
    "  - A **bias** $b$\n",
    "- Input $x \\in \\mathbb{R}$, output $y \\in \\mathbb{R}$\n",
    "\n",
    "This is a highly simplified network, but it reveals key behaviors that extend to real-world models.\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Forward Pass Dynamics\n",
    "\n",
    "The output of layer 1:  \n",
    "$y_1 = wx + b$\n",
    "\n",
    "The output of layer 2:  \n",
    "$y_2 = w(wx + b) + b = w^2x + wb + b$\n",
    "\n",
    "In general, at the $n$th layer:  \n",
    "$y_n = w^n x + \\left( \\frac{1 - w^n}{1 - w} \\right) b$\n",
    "\n",
    "(Assuming all $w_i = w$ and $b_i = b$)\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Activation Behavior by Weight Magnitude\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/412.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "### Case 1: $w < 1$\n",
    "- $w^n \\rightarrow 0$ as $n \\rightarrow \\infty$\n",
    "- Activations vanish: network **forgets the input**\n",
    "- Output becomes dominated by bias instead of actual input\n",
    "- Known as **vanishing input**\n",
    "\n",
    "### Case 2: $w = 1$\n",
    "- $y_n = x + nb$ (ideal)\n",
    "- Input preserved, but sensitive to initialization\n",
    "- Extremely rare in practice\n",
    "\n",
    "### Case 3: $w > 1$\n",
    "- $w^n \\rightarrow \\infty$ as $n$ increases\n",
    "- **Activations explode**\n",
    "- Outputs become `inf` or `nan`\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Backward Pass Dynamics (Gradients)\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/413.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "During backpropagation, gradients behave similarly:\n",
    "\n",
    "- Gradients are also multiplied by $w$ at each layer\n",
    "- So they **vanish** if $w < 1$\n",
    "- They **explode** if $w > 1$\n",
    "\n",
    "This leads to:\n",
    "- **Vanishing gradients**: nothing updates\n",
    "- **Exploding gradients**: model diverges\n",
    "\n",
    "Usually in practice, activations will explode before your gradients, causing the output of your network to be NaN, which will cause all of the weights in the network to be set to NaN during backpropagation. However, exploding gradients do exist, and they are hard to control if the network was not initialized properly.\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Summary of Effects\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/414.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "| Weight $w$ | Activations | Gradients | Outcome |\n",
    "|------------|-------------|-----------|---------|\n",
    "| $< 1$      | Vanish      | Vanish    | Network is stable, but learns nothing |\n",
    "| $= 1$      | Stable      | Stable    | Ideal but unlikely |\n",
    "| $> 1$      | Explode     | Explode   | Network crashes |\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Handling Exploding Gradients\n",
    "\n",
    "### General Info\n",
    "\n",
    "Exploding gradients occur when gradients grow exponentially during backpropagation, often due to large weights or overly large learning rates.  \n",
    "This leads to unstable training, and eventually `inf` or `nan` values in the weights.\n",
    "\n",
    "- Vanishing/exploding gradients were major problems in **recurrent networks (RNNs)**\n",
    "- Now occur in **very deep feedforward networks**\n",
    "- Most modern frameworks (e.g., PyTorch) use smart initializations to reduce the risk\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/415.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/416.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "### Key Symptoms\n",
    "\n",
    "- Sudden spikes in loss or rapid increase in loss to `inf`\n",
    "- Model output becomes `nan` or `inf`\n",
    "- Network crashes or throws numerical errors\n",
    "\n",
    "### Diagnosis\n",
    "\n",
    "- **Plot gradient norms** and **weight norms** per layer\n",
    "- Check for:\n",
    "  - Extremely large values\n",
    "  - Appearance of `inf` or `nan`\n",
    "- Try training with **learning rate = 0**:\n",
    "  - If the loss still fluctuates, the variation is due to the dataset, not learning\n",
    "  - If it becomes `nan` during actual training, it’s likely an exploding gradient problem\n",
    "\n",
    "### Remedy\n",
    "\n",
    "- Lower the **learning rate**\n",
    "- Use **gradient clipping** (not covered yet)\n",
    "- Check or improve **weight initialization**  \n",
    "- Avoid very large initial weights or overly deep unregularized layers\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Handling Vanishing Gradients\n",
    "\n",
    "### General Info\n",
    "\n",
    "Vanishing gradients occur when gradients shrink exponentially during backpropagation.  \n",
    "As a result, early layers learn very slowly or not at all.\n",
    "\n",
    "This is an especially common problem in deep networks where the weight norms are small (e.g., < 1).\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/417.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/418.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "### Key Symptoms\n",
    "\n",
    "- Loss decreases slightly at first, then plateaus\n",
    "- Network output relies only on the final layer (bias terms)\n",
    "- Gradients in earlier layers approach zero\n",
    "- No meaningful learning beyond shallow layers\n",
    "\n",
    "### Diagnosis\n",
    "\n",
    "- Plot **gradient norms** per layer — early layers will have gradients near zero\n",
    "- Try a **learning rate of 0** and compare baseline loss fluctuation\n",
    "- Compare weight and gradient flow across layers\n",
    "\n",
    "### Remedy\n",
    "\n",
    "This happens to all but the shallowest network, so there is really not a direct remedy.\n",
    "\n",
    "- Change the **network structure** — vanilla deep networks will not work\n",
    "- Slightly increase learning rate if gradients are extremely small\n",
    "- Use **normalization techniques** (e.g., BatchNorm)\n",
    "- Add **residual connections** to preserve gradient flow\n",
    "- Adjust weight **initialization** (e.g., Xavier or He initialization)\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Final Notes\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/419.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "- Vanishing gradients: common, require architectural changes\n",
    "- Exploding gradients: rarer, often fixed with learning rate reduction\n",
    "- Solutions (covered next):\n",
    "  - **Normalization**\n",
    "  - **Residual connections**\n",
    "  - Better initialization\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
