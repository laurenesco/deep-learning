{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16988368-4858-43bd-97ec-6b135d778986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4314]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# We've talked about what deep networks are and the role of activation functions.\n",
    "# Now let's see how to implement a simple deep network using PyTorch.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Any deep network should be implemented as a class that inherits from nn.Module\n",
    "class SimpleNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Always call super().__init__() first to initialize the parent class.\n",
    "        # PyTorch uses this to set up internal tracking for parameters, modules, etc.\n",
    "        super().__init__()\n",
    "\n",
    "        # Define components of your network (with learnable parameters) in __init__.\n",
    "        # These will be automatically registered and tracked.\n",
    "        self.fc1 = nn.Linear(1, 10)     # First fully connected layer (input size 1 → 10)\n",
    "        self.relu = nn.ReLU()           # Nonlinear activation\n",
    "        self.fc2 = nn.Linear(10, 1)     # Second layer (hidden size 10 → output size 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # This is the forward pass. It defines how input data flows through the network.\n",
    "        x = self.fc1(x)     # First linear transformation\n",
    "        x = self.relu(x)    # Activation\n",
    "        x = self.fc2(x)     # Second transformation\n",
    "        return x            # Final output tensor\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleNetwork()\n",
    "\n",
    "# Call the model with an input tensor (batch of size 1, feature size 1)\n",
    "input_tensor = torch.tensor([[1.0]])\n",
    "output = model(input_tensor)\n",
    "\n",
    "# Output is the transformed tensor after passing through Linear → ReLU → Linear\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38c721ec-d2c3-47a6-8f60-e3000004fc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5868]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# PyTorch makes building custom networks flexible.\n",
    "# You can easily extend the structure to accept multiple inputs.\n",
    "\n",
    "class TwoInputNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define separate linear layers for each input\n",
    "        self.fc1_x = nn.Linear(1, 10)\n",
    "        self.fc1_y = nn.Linear(1, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        # Apply a linear layer and activation to both inputs independently\n",
    "        x = self.relu(self.fc1_x(x))\n",
    "        y = self.relu(self.fc1_y(y))\n",
    "\n",
    "        # Combine the two paths and pass through final layer\n",
    "        z = x + y\n",
    "        z = self.fc2(z)\n",
    "        return z\n",
    "\n",
    "# Instantiate and call the two-input model\n",
    "model2 = TwoInputNetwork()\n",
    "\n",
    "x_input = torch.tensor([[1.0]])\n",
    "y_input = torch.tensor([[2.0]])\n",
    "result = model2(x_input, y_input)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e785a7e-65ea-4fcb-accf-af83abf889db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5868, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you want to compute gradients for a backward pass (e.g., during training):\n",
    "loss = result.sum()         # Dummy scalar loss\n",
    "loss.backward()             # Compute gradients for all model parameters\n",
    "\n",
    "loss\n",
    "# Gradients are now available in model2.parameters()\n",
    "# No manual bookkeeping required — PyTorch handles the backward graph for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bbd6a7f-b071-439c-b0bc-dcd5a77ebd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary:\n",
    "# - Define layers in __init__.\n",
    "# - Use nn.Module components (e.g., Linear, ReLU).\n",
    "# - Forward pass builds the computation graph.\n",
    "# - Call .backward() to compute gradients.\n",
    "# PyTorch handles all parameter tracking and gradient propagation under the hood."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
