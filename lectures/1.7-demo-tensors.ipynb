{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e33fcff0-504c-48ce-9dff-9a50d4f66842",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Tensor Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a3de9f-c75f-478d-a724-620f3ed72d9f",
   "metadata": {},
   "source": [
    "#### Analogy\n",
    "Imagine a library:\n",
    "\n",
    "- **Scalar**: A single book.\n",
    "- **Vector**: A shelf with a row of books.\n",
    "- **Matrix**: A bookshelf with rows (shelves) and columns (positions on each shelf).\n",
    "- **Tensor**: A whole library building with multiple floors, each containing several bookshelves. Each bookâ€™s location can be identified by specifying the floor, shelf, and position on the shelf. This layered structure is similar to how tensors work in multiple dimensions.\n",
    "\n",
    "#### Notes\n",
    "Tensors are kind of like rigid and strictly defined multidimensional hashes, at least as far as indexing goes. This is a very naive comparison, but one very useful for my understanding. For example, a perl hash can read like $HASH->{floor}->{shelf}->{position}.\n",
    "\n",
    "Tensors are a type of multidimensional array that abides by mathematical rules to ensure they capture intrinsic geometric or physical relationships no matter the coordinate system viewed in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a5af34-cb17-496e-9cd0-c43ba9824470",
   "metadata": {},
   "source": [
    "## Important tensor functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4662a50d-6e40-4c1a-ace4-fd4a55facc3d",
   "metadata": {},
   "source": [
    "#### Descriptive analysis\n",
    "Given a tensor defined as `t = torch.tensor([1, 2, 3])`\n",
    "\n",
    "- `t.ndim` - number of dimensions\n",
    "- `t.shape` - shape of the tensor\n",
    "- `t.dtype` - data type of tensor\n",
    "\n",
    "#### Tensor construction\n",
    "- `t = torch.tensor([1, 2, 3], dtype=torch.float32)` - set data type to float **almost always want to do this**\n",
    "- `t = torch.zeros(3)` - create a 0 tensor of size 3\n",
    "- `t = torch.ones(3)` - create tensor of 1's of size 3\n",
    "- `t = torch.rand(3,3,3)` - create a 3x3x3 tensor of random values\n",
    "- `t = torch.arange(10)` - create a tensor of 10 elements that go from 0 to 10\n",
    "- Create a tensor from an image:\n",
    "  ```\n",
    "  from PIL import Image\n",
    "  import numpy\n",
    "  \n",
    "  img = Image.open('cat.jpg')\n",
    "  torch.as_tensor(np.array(img)) // as_tensor likes numpy arrays\n",
    "  ```\n",
    "\n",
    "#### Basic tensor operations\n",
    "Given tensors defined as:\n",
    "`a = torch.ones(10)`\n",
    "`b = torch.arange(10)`\n",
    "\n",
    "- `a + b` - add the tensors (same for subtract, multiply, divide, and powers)\n",
    "\n",
    "**Tensors must be the same size for these operations to work**\n",
    "\n",
    "#### Linear algebra\n",
    "- `t.mT` - transpose the tensor\n",
    "- `t.permute(1, 2, 0)` - permute the tensor (like transpose but for multiple dimensions)\n",
    "\n",
    "#### Other useful functions\n",
    "- `torch.as_tensor(np.array(var))` - convert array into tensor\n",
    "- `t.view(2,3)` - view a tensor as a different shape (this is for a size 6 tensor)\n",
    "- `t.reshape(2,3)` - copies underlying data and view as a different shape\n",
    "- `%timeit` - times whatever operation follows\n",
    "- `.to('cuda')` - allows you to offload computation to a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e417ac37-608f-462e-b358-ca73037bef3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.tensor([1, 2, 3])\n",
    "print(t.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286470b1-b9a1-4f30-947a-d79d319a7cf9",
   "metadata": {},
   "source": [
    "## Manipulating Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b212bfa-44e6-47cf-8a4f-ea78f538bee2",
   "metadata": {},
   "source": [
    "### Reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c80ebf82-2e5a-4b87-961b-361157e0574f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will start by creating a tensor with six elements..\n",
      "\n",
      "We can see that the shape of the tensor is just 6 using a.shape: \n",
      "\ttorch.Size([6])\n",
      "\n",
      "But we can also view this tensor as 2x3 using a.view(2,3) or a.reshape(2,3): \n",
      "\ttensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "\n",
      "The difference between the two is that view will not copy the underlying data, it will simply change the shape. Reshape will copy the underlying data.\n",
      "\n",
      "\n",
      "Notice the size of a remains 6 after using view(): torch.Size([6])\n",
      "And after using reshape: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "print(\"We will start by creating a tensor with six elements..\\n\")\n",
    "\n",
    "a = torch.arange(6)\n",
    "print(f\"We can see that the shape of the tensor is just 6 using a.shape: \\n\\t{a.shape}\\n\")\n",
    "print(f\"But we can also view this tensor as 2x3 using a.view(2,3) or a.reshape(2,3): \\n\\t{a.view(2,3)}\")\n",
    "\n",
    "print(\"\\nThe difference between the two is that view will not copy the underlying data, \"\n",
    "      \"it will simply change the shape. Reshape will copy the underlying data.\")\n",
    "\n",
    "print(f\"\\n\\nNotice the size of a remains 6 after using view(): {a.size()}\")\n",
    "a.reshape(2,3)\n",
    "print(f\"And after using reshape: {a.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb4f33a-c1c7-4add-b69a-4106f94863ea",
   "metadata": {},
   "source": [
    "### Transposing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6094a00c-e871-416c-8ceb-32db97e82d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next we will look at some basic linear algebra, starting with transposing..\n",
      "\n",
      "Starting with a random tensor of size 2, 3: torch.Size([2, 3])\n",
      "tensor([[0.9049, 0.9831, 0.1013],\n",
      "        [0.7978, 0.7693, 0.9045]])\n",
      "\n",
      "The transpose:\n",
      "tensor([[0.9049, 0.7978],\n",
      "        [0.9831, 0.7693],\n",
      "        [0.1013, 0.9045]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Next we will look at some basic linear algebra, starting with transposing..\\n\")\n",
    "\n",
    "a = torch.rand(2,3)\n",
    "print(f\"Starting with a random tensor of size 2, 3: {a.size()}\")\n",
    "print(f\"{a}\")\n",
    "print(f\"\\nThe transpose:\\n{a.mT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df56789-077d-4e40-8dbd-2ffe704a6f9a",
   "metadata": {},
   "source": [
    "### Permuting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "da471be7-ac55-4a22-87e6-ac418ff04dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And we will also look at permutations, which allow us to transpose along any dimension\n",
      "\n",
      "Starting with a random tensor of size 2, 3, 4: torch.Size([2, 3, 4])\n",
      "tensor([[[0.4102, 0.6469, 0.8001, 0.8156],\n",
      "         [0.1233, 0.5795, 0.5240, 0.3115],\n",
      "         [0.0099, 0.0912, 0.5016, 0.7316]],\n",
      "\n",
      "        [[0.4813, 0.9499, 0.1991, 0.5247],\n",
      "         [0.0276, 0.6686, 0.7974, 0.6779],\n",
      "         [0.0204, 0.6619, 0.7100, 0.4141]]])\n",
      "\n",
      "The transpose using permutation:\n",
      "tensor([[[0.4102, 0.4813],\n",
      "         [0.6469, 0.9499],\n",
      "         [0.8001, 0.1991],\n",
      "         [0.8156, 0.5247]],\n",
      "\n",
      "        [[0.1233, 0.0276],\n",
      "         [0.5795, 0.6686],\n",
      "         [0.5240, 0.7974],\n",
      "         [0.3115, 0.6779]],\n",
      "\n",
      "        [[0.0099, 0.0204],\n",
      "         [0.0912, 0.6619],\n",
      "         [0.5016, 0.7100],\n",
      "         [0.7316, 0.4141]]])\n",
      "\n",
      "The shape of the permuted tensor is: torch.Size([3, 4, 2])\n",
      "\n",
      "a.permute(1,2,0) takes the data originally stored in dimension 3 and stores it in \n",
      "dimension 2, 2 to 1, and 1 to 3. So we can see how we sort of manually reordered \n",
      "the dimensions.\n"
     ]
    }
   ],
   "source": [
    "print(\"And we will also look at permutations, which allow us to transpose along any dimension\\n\")\n",
    "\n",
    "a = torch.rand(2,3,4)\n",
    "print(f\"Starting with a random tensor of size 2, 3, 4: {a.size()}\")\n",
    "print(f\"{a}\")\n",
    "print(f\"\\nThe transpose using permutation:\\n{a.permute(1,2,0)}\")\n",
    "print(f\"\\nThe shape of the permuted tensor is: {a.permute(1,2,0).shape}\")\n",
    "print(\"\\na.permute(1,2,0) takes the data originally stored in dimension 3 and stores it in \\ndimension 2, 2 to 1, and 1 to 3. So we can see how we sort of manually reordered \\nthe dimensions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61398917-52bc-4137-ac43-4a0c68f11818",
   "metadata": {},
   "source": [
    "### The Singleton Dimension\n",
    "\n",
    "The singleton dimension is a dimension with size 1. It acts as a placeholder, and can be expanded to match the size of another tensors corresponding dimension during **broadcasting**. \n",
    "\n",
    "With broadcasting, tensors with the following dimensions can have basic math operations performed on them:\n",
    "- (3,3,1,3,1,1)\n",
    "- (3,3,5,3,4,7)\n",
    "  \n",
    "This is because the singleton dimensions will broadcast (copy) its existing values across it as many times as needed, since the dimension is effectively empty.\n",
    "\n",
    "**Using the singleton dimension in this way is essentially using the outer product**\n",
    "\n",
    "### Conceptualizing higher dimensional tensors\n",
    "Consider a tensor of shape (3,3,5,3,4,7), this tensor has six axes:\n",
    "- Axis 0 (size 3): You have 3 blocks (like 3 independent groups).\n",
    "- Axis 1 (size 3): Each block contains 3 matrices.\n",
    "- Axis 2 (size 5): Each matrix consists of 5 slices.\n",
    "- Axis 3 (size 3): Each slice contains 3 rows.\n",
    "- Axis 4 (size 4): Each row consists of 4 columns.\n",
    "- Axis 5 (size 7): Each column has 7 values.\n",
    "\n",
    "We can visualize this like a warehouse:\n",
    "- There are 3 floors â†’ axis 0\n",
    "- Each floor has 3 aisles â†’ axis 1\n",
    "- Each aisle has 5 shelves â†’ axis 2\n",
    "- Each shelf has 3 rows of bins â†’ axis 3\n",
    "- Each row of bins contains 4 columns of bins â†’ axis 4\n",
    "- Each bin holds 7 individual items â†’ axis 5\n",
    "\n",
    "No real standard naming exists outside of row and column for 2D tensors, slice or depth is often used for 3D tensors. For 4D, in ML we often use batch, channel, or feature map. Beyond that nobody really works with the tensors explicitly, so best to refer to them by axis beyond 4 dimensions.\n",
    "\n",
    "**Each dimension of a tensor can be thought of as a container for the next**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa120852-2803-4e19-85a2-b5db34f445c5",
   "metadata": {},
   "source": [
    "### Adding Singleton Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "235c0553-3a0b-4f96-9f38-314cee7b3ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with a random tensor of size 6\n",
      "\n",
      "tensor([0, 1, 2, 3, 4, 5])\n",
      "\n",
      "We can add a dimension of size 1 using the keyword None and indexing where we want this \n",
      "dimension to be. For example,a[None] puts the dimension at the beginning, and \n",
      "a[:,None] at the end. \n",
      "\n",
      "tensor([[0, 1, 2, 3, 4, 5]])\n",
      "\n",
      "Notice the shape of a with the dimension added: torch.Size([1, 6])\n",
      "\n",
      "Another example using a = torch.arange(6).view(3,2), a[None, :, :, None].shape:\n",
      "torch.Size([1, 3, 2, 1])\n",
      "\n",
      "Another way to index the very end of a tensor is a[..., None].\n",
      "\n",
      "P.S. Apparently a.unsqueeze(idx) works too.\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(6)\n",
    "print(f\"Starting with a random tensor of size 6\\n\\n{a}\\n\\nWe can add a dimension of size 1 using\"\n",
    "      f\" the keyword None and indexing where we want this \\ndimension to be. For example,\"\n",
    "      f\"a[None] puts the dimension at the beginning, and \\na[:,None] at the end. \\n\\n{a[None]}\"\n",
    "      f\"\\n\\nNotice the shape of a with the dimension added: {a[None].shape}\")\n",
    "\n",
    "# Another example with more dimensions\n",
    "print(\"\\nAnother example using a = torch.arange(6).view(3,2), a[None, :, :, None].shape:\")\n",
    "a = torch.arange(6).view(3,2)\n",
    "print (a[None, :, :, None].shape)\n",
    "\n",
    "print(\"\\nAnother way to index the very end of a tensor is a[..., None].\")\n",
    "\n",
    "print(\"\\nP.S. Apparently a.unsqueeze(idx) works too.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1d6bb2-7d8f-428d-8408-4abed914629e",
   "metadata": {},
   "source": [
    "### Removing Singleton Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d501ba50-84fd-408a-af4d-8ec29d7f03c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We can also remove singleton dimensions using the squeeze function. Let's start\n",
      "with a tensor of the following shape: \n",
      "\n",
      "torch.Size([3, 2, 1, 1]). \n",
      "\n",
      "We can certainly remove a dimension by settingit to 0 using: a[..., 0], but this is \n",
      "not ideal.\n",
      "\n",
      "We can use squeeze to completely remove the last dimension with a.squeeze(idx), where \n",
      "idx is the index of the dimensionto remove. a with the last dimension removed: \n",
      "\n",
      "torch.Size([3, 2, 1])\n",
      "\n",
      "Squeeze, when called with no arguments, removes all singleton dimensions in the tensor. \n",
      "This is very dangerous and should never be done.\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(6).view(3,2,1,1)\n",
    "\n",
    "print(f\"We can also remove singleton dimensions using the squeeze function. Let's start\\nwith\"\n",
    "      f\" a tensor of the following shape: \\n\\n{a.shape}. \\n\\nWe can certainly remove a dimension by setting\"\n",
    "      f\"it to 0 using: a[..., 0], but this is \\nnot ideal.\\n\\nWe can use squeeze to completely\"\n",
    "      f\" remove the last dimension with a.squeeze(idx), where \\nidx is the index of the dimension\"\n",
    "      f\"to remove. a with the last dimension removed: \\n\\n{a.squeeze(-1).shape}\")\n",
    "\n",
    "print(\"\\nSqueeze, when called with no arguments, removes all singleton dimensions in the\"\n",
    "      \" tensor. \\nThis is very dangerous and should never be done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b3a14f-0c54-4768-baa1-17640c940065",
   "metadata": {},
   "source": [
    "### Broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cf475e5f-7191-46d3-8928-774af352b3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using broadcasting, we can add (or perform other basic math) on multiple tensors that are \n",
      "not the same size, but the size difference is on the axis for which one of the tensors \n",
      "has a singleton dimension.\n",
      "\n",
      "\n",
      "a's shape = torch.Size([4, 1])\n",
      "b's shape = torch.Size([4, 5])\n",
      "\n",
      "\n",
      "tensor([[1.4773, 1.1399, 0.6014, 1.3290, 1.5759],\n",
      "        [1.1367, 1.5279, 1.3871, 1.5639, 0.7121],\n",
      "        [1.5028, 1.2352, 1.5918, 1.1748, 1.4737],\n",
      "        [0.6673, 1.1397, 1.2079, 1.0231, 1.1857]])\n",
      "\n",
      "\n",
      "An illustrative example:\n",
      "\n",
      "a's shape = torch.Size([4, 1])\n",
      "b's shape = torch.Size([1, 5])\n",
      "\n",
      "\n",
      "tensor([[ 0, 10, 20, 30, 40],\n",
      "        [ 1, 11, 21, 31, 41],\n",
      "        [ 2, 12, 22, 32, 42],\n",
      "        [ 3, 13, 23, 33, 43]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using broadcasting, we can add (or perform other basic math) on multiple tensors\"\n",
    "      f\" that are \\nnot the same size, but the size difference is on the axis for which\"\n",
    "      f\" one of the tensors \\nhas a singleton dimension.\\n\\n\")\n",
    "\n",
    "a = torch.rand(4,1)\n",
    "b = torch.rand(4,5)\n",
    "\n",
    "print(f\"a's shape = {a.shape}\")\n",
    "print(f\"b's shape = {b.shape}\")\n",
    "print(\"\\n\")\n",
    "print(a + b)\n",
    "\n",
    "print(\"\\n\\nAn illustrative example:\\n\")\n",
    "\n",
    "a = torch.arange(4).view(4,1)\n",
    "b = torch.arange(5).view(1,5) * 10\n",
    "\n",
    "print(f\"a's shape = {a.shape}\")\n",
    "print(f\"b's shape = {b.shape}\")\n",
    "print(\"\\n\")\n",
    "print(a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "835088a0-7639-4cb0-ba95-8e34fbebbee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a clever use of broadcasting to calculate pairwise squared Euclidean \n",
      "distances between rows of x\n",
      "\n",
      "\n",
      "tensor([[ 0.0000,  7.4260, 13.2203, 22.2614, 13.7267, 10.3759,  2.8846, 11.7383,\n",
      "         15.2080, 19.5554],\n",
      "        [ 7.4260,  0.0000,  0.8759,  4.7477,  1.7006,  0.2576,  3.3030,  0.7801,\n",
      "          1.6822,  4.3813],\n",
      "        [13.2203,  0.8759,  0.0000,  1.7125,  0.5600,  0.1841,  6.2261,  0.1886,\n",
      "          0.8633,  1.9291],\n",
      "        [22.2614,  4.7477,  1.7125,  0.0000,  1.0380,  2.9086, 10.8271,  1.7488,\n",
      "          3.7939,  0.3338],\n",
      "        [13.7267,  1.7006,  0.5600,  1.0380,  0.0000,  0.8927,  5.2269,  0.1936,\n",
      "          2.7968,  0.6276],\n",
      "        [10.3759,  0.2576,  0.1841,  2.9086,  0.8927,  0.0000,  4.7240,  0.2576,\n",
      "          0.9868,  2.8763],\n",
      "        [ 2.8846,  3.3030,  6.2261, 10.8271,  5.2269,  4.7240,  0.0000,  4.6168,\n",
      "          9.6616,  8.3670],\n",
      "        [11.7383,  0.7801,  0.1886,  1.7488,  0.1936,  0.2576,  4.6168,  0.0000,\n",
      "          1.7984,  1.4749],\n",
      "        [15.2080,  1.6822,  0.8633,  3.7939,  2.7968,  0.9868,  9.6616,  1.7984,\n",
      "          0.0000,  4.8952],\n",
      "        [19.5554,  4.3813,  1.9291,  0.3338,  0.6276,  2.8763,  8.3670,  1.4749,\n",
      "          4.8952,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Here is a clever use of broadcasting to calculate pairwise squared Euclidean \\ndistances\"\n",
    "     \" between rows of x\\n\\n\")\n",
    "\n",
    "x = torch.randn(10,2)\n",
    "d = torch.zeros(10,10)\n",
    "\n",
    "d = (x[:, None, :] - x[None, :, :]).pow(2).sum(-1)\n",
    "\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c772985e-ee35-4e3b-9fa3-97afd883c73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a clever use of broadcasting to calculate the maximum distance between \n",
      "two points in x\n",
      "\n",
      "\n",
      "tensor(17.9494) (tensor(3), tensor(8))\n"
     ]
    }
   ],
   "source": [
    "print(\"Here is a clever use of broadcasting to calculate the maximum distance between \"\n",
    "      \"\\ntwo points in x\\n\\n\")\n",
    "\n",
    "x = torch.randn(10,2)\n",
    "d = torch.zeros(10,10)\n",
    "\n",
    "d = (x[:, None, :] - x[None, :, :]).pow(2).sum(-1)\n",
    "\n",
    "print(d.max(), (d.argmax() // 10, d.argmax() %10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
