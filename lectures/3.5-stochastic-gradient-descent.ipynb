{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d7ba6d3-f1b4-46f3-be65-93da0174fd33",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent (SGD)\n",
    "\n",
    "## Introduction\n",
    "- **Stochastic Gradient Descent (SGD)** is a fundamental optimization algorithm in deep learning.\n",
    "- It enables efficient training on large datasets by updating model parameters more frequently than standard gradient descent.\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset Setup\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/flwrs.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "- For this class: a **dataset** is a set of input-output pairs:\n",
    "  $$\n",
    "  \\{(x_i, y_i)\\}_{i=1}^N\n",
    "  $$\n",
    "  where $x_i$ is the input, $y_i$ is the corresponding label.\n",
    "\n",
    "---\n",
    "\n",
    "## Model and Parameters\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/mdl.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "- A deep network is a composition of **linear and nonlinear layers**.\n",
    "- Certain layers contain **parameters** $\\theta$ (weights and biases).\n",
    "- Deep networks are differentiable, allowing us to compute gradients w.r.t. $\\theta$.\n",
    "\n",
    "---\n",
    "\n",
    "## Loss Functions\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/lss.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/trn.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "- Loss functions tie inputs $x$ to labels $y$ by measuring how wrong the model is.\n",
    "- Two forms:\n",
    "  - **Individual loss**: $l(f_\\theta(x), y)$\n",
    "  - **Expected loss**:  \n",
    "    $L(\\theta) = \\mathbb{E}[l(f_\\theta(x), y)]$\n",
    "\n",
    "### Goal of Training:\n",
    "Find parameters $\\theta$ that minimize expected loss:  \n",
    "$\\theta^* = \\arg\\min_\\theta L(\\theta)$\n",
    "\n",
    "---\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/gds.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/gdi.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "### Update Rule:\n",
    "$$\n",
    "\\theta = \\theta - \\epsilon [\\nabla_\\theta L(\\theta)]^\\top\n",
    "$$\n",
    "- $\\epsilon$ is the learning rate.\n",
    "\n",
    "\n",
    "### Limitation:\n",
    "- Requires looping through the **entire dataset** before a single parameter update.\n",
    "- Becomes very slow with large datasets or large models.\n",
    "  \n",
    "---\n",
    "\n",
    "## Stochastic Gradient Descent (SGD)\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/sgd.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "### Key Insight:\n",
    "SGD performs an update **after each sample**, not after the entire dataset.\n",
    "\n",
    "### Pseudocode (SGD):\n",
    "\n",
    "for epoch in range(num_epochs):  \n",
    "  for $(x_i, y_i)$ in dataset:  \n",
    "    grad = compute_gradient($f_\\theta(x_i)$, $y_i$)  \n",
    "    $\\theta -= \\epsilon \\cdot \\text{grad}$\n",
    "\n",
    "### Why It Works:\n",
    "- Introduces **noise**, but allows more frequent updates.\n",
    "- Each gradient is an unbiased estimate of the true gradient.\n",
    "\n",
    "---\n",
    "\n",
    "## Behavior Comparison\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/gvs.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "**Gradient Descent**  \n",
    "- Smooth loss curve  \n",
    "- Stable updates  \n",
    "- Computationally expensive  \n",
    "\n",
    "**Stochastic Gradient Descent**  \n",
    "- Noisy loss curve  \n",
    "- Frequent updates  \n",
    "- Much faster iteration  \n",
    "- Still converges in expectation\n",
    "\n",
    "---\n",
    "\n",
    "## Why Does SGD Fluctuate?\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/sgdd.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "1. **Loss values vary** across different samples  \n",
    " → Loss curve is noisy  \n",
    "2. **Gradient directions vary** across samples  \n",
    " → Occasional steps in the wrong direction\n",
    "\n",
    "Even without updates (set $\\epsilon = 0$), plotting individual losses across samples is jumpy.\n",
    "\n",
    "---\n",
    "\n",
    "## Visual Example of SGD\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/gex.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "- Start at a random $\\theta$\n",
    "- Use the red sample → take one gradient step\n",
    "- Use green sample → take another step\n",
    "- Use purple sample → another step\n",
    "- Some steps may help, some may hurt\n",
    "- But on average, SGD finds its way toward the minimum\n",
    "\n",
    "---\n",
    "\n",
    "## Convergence\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/gvss.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "### Case 1: All Gradients Align  \n",
    "- Low variance  \n",
    "- SGD behaves like GD  \n",
    "- Converges much faster\n",
    "\n",
    "### Case 2: Gradients Differ  \n",
    "- High variance  \n",
    "- Convergence speed depends on the variance of the gradient estimate\n",
    "\n",
    "## Reducing Variance\n",
    "- Much of optimization research in deep learning is about:\n",
    "  - **Reducing gradient variance**\n",
    "  - Making SGD converge more reliably and quickly\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "- **SGD** is much faster than full gradient descent, especially for large datasets\n",
    "- **Convergence speed depends on gradient variance**\n",
    "- Many improvements to SGD aim to reduce variance (we’ll study those later)\n",
    "\n",
    "> In short:  \n",
    "> SGD = faster updates + noisy gradients, but it works — and it’s the reason deep learning is practical today."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
