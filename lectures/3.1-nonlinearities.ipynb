{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fdb8c59-26f9-4dce-8548-99bd5821ad95",
   "metadata": {},
   "source": [
    "# Non-Linearity and Deep Networks\n",
    "\n",
    "## Motivation: Why Linear Models Are Limited\n",
    "- Linear models struggle with distinguishing non-linear patterns, e.g., a dog paw on gray background.\n",
    "- They are incapable of modeling distinctions where the relevant features are symmetric or inverted versions (like black vs. white paws).\n",
    "\n",
    "## The Solution: Add Non-Linearity\n",
    "- Non-linear models overcome the expressive limitations of linear models.\n",
    "- **Rectified Linear Unit (ReLU)** is the simplest and most common non-linearity:\n",
    "  \n",
    "  $$\n",
    "  \\text{ReLU}(x) = \\max(0, x)\n",
    "  $$\n",
    "\n",
    "<img src=\"./images/relu.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "- ReLU is piecewise linear:\n",
    "  - Identity for \\( x > 0 \\)\n",
    "  - Constant zero for \\( x < 0 \\)\n",
    "  - Gradient:\n",
    "    - 0 for \\( x < 0 \\)\n",
    "    - 1 for \\( x > 0 \\)\n",
    "    - Undefined at \\( x = 0 \\), but this is ignored in practice\n",
    "\n",
    "## Constructing a Simple Non-Linear Network\n",
    "- Add a ReLU between two linear layers → enables the network to distinguish features like black vs. white pixels.\n",
    "\n",
    "<img src=\"./images/relu-paw.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "- Intuition:\n",
    "  - Use one neuron to detect \"brighter than gray\" (\\( x - 0.5 \\))\n",
    "  - Use another to detect \"darker than gray\" (\\( 0.5 - x \\))\n",
    "  - Apply ReLU to both outputs\n",
    "  - Sum the absolute deviations from gray and subtract a constant\n",
    "\n",
    "### Mathematical Example\n",
    "- Assign pixel values:\n",
    "  - Black = 0\n",
    "  - Gray = 0.5\n",
    "  - White = 1\n",
    "- First layer outputs:\n",
    "  - \\( x - 0.5 \\)\n",
    "  - \\( 0.5 - x \\)\n",
    "- After ReLU:\n",
    "  - Compute \\( |x - 0.5| \\)\n",
    "- Subtract a constant (e.g., 0.25) to normalize gray to 0\n",
    "- This network can now distinguish black/white paws from gray background\n",
    "\n",
    "## Deep Network Structure\n",
    "- Deep networks are built by alternating **linear transformations** and **non-linearities**.\n",
    "- Each layer maps from an \\( n \\)-dimensional input to a \\( c \\)-dimensional output:\n",
    "  \n",
    "  $$\n",
    "  f(x) = \\text{ReLU}(W_2(\\text{ReLU}(W_1x + b_1)) + b_2)\n",
    "  $$\n",
    "\n",
    "- Parameters (weights and biases) are learned for **linear transformations**.\n",
    "- Non-linearities usually don’t have learnable parameters (though some variants do).\n",
    "\n",
    "<img src=\"./images/relu-lin.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "## What Is a Layer?\n",
    "- A **layer** is a reusable computational block.\n",
    "- Examples:\n",
    "  - Linear Layer: \\( Wx + b \\)\n",
    "  - Non-Linearity: ReLU, Sigmoid, etc.\n",
    "- Deep networks are composed of these elementary blocks.\n",
    "- Common stacking pattern:\n",
    "  - Linear → ReLU → Linear → ReLU → ...\n",
    "\n",
    "<img src=\"./images/layer.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "## Counting Layers\n",
    "- \"10-layer network\" = 10 linear layers (non-linearities are not counted).\n",
    "- In complex architectures:\n",
    "  - Count the **maximum number of sequential linear transformations** an input goes through.\n",
    "  - Parallel layers do **not** increase the layer count.\n",
    "\n",
    "<img src=\"./images/layer-count.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "## Universal Approximation Theorem\n",
    "- A network with just:\n",
    "  - One hidden layer (with a non-linearity)\n",
    "  - And a linear output layer\n",
    "  - Can approximate **any continuous function** (under certain conditions)\n",
    "- Implications:\n",
    "  - Deep networks are **arbitrarily expressive**\n",
    "  - But...\n",
    "\n",
    "<img src=\"./images/uat.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "### Caveats\n",
    "1. **Training may be difficult** — finding the right parameters is hard\n",
    "2. **Construction may be inefficient** — theoretical networks might require excessive resources\n",
    "\n",
    "## Practical Deep Learning Focus\n",
    "- Instead of proving expressivity, we build **efficient architectures** that can:\n",
    "  - Express the functions we care about\n",
    "  - Be trained effectively on real data\n",
    "- Non-linearities are what make deep networks **powerful** and **expressive**\n",
    "\n",
    "<img src=\"./images/dntldr.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
