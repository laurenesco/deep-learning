{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eec900d5-cf6d-4495-9c20-e877eb04f060",
   "metadata": {},
   "source": [
    "# Lecture Notes: Variance Reduction in Stochastic Gradient Descent (SGD)\n",
    "\n",
    "## Recap: What is Stochastic Gradient Descent?\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/391.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "Stochastic Gradient Descent (SGD) is a widely used optimization algorithm in deep learning.\n",
    "\n",
    "- It updates model parameters **after computing the gradient of the loss** with respect to a **single sample or batch**.\n",
    "- This introduces **noise** into the optimization path, which can be helpful for escaping local minima but also slows convergence.\n",
    "- SGD trades off precision in gradient estimation for **faster updates** and **lower memory cost** compared to full-batch gradient descent.\n",
    "\n",
    "## The Problem: High Gradient Variance\n",
    "\n",
    "The convergence rate of SGD is roughly proportional to the variance of the gradient estimates.\n",
    "\n",
    "- If the gradients computed on different samples are all pointing in **roughly the same direction**, the optimizer moves efficiently — this is **low variance**.\n",
    "- If those gradients **disagree widely**, the optimizer \"jumps around\" and converges more slowly — this is **high variance**.\n",
    "\n",
    "This variance limits how quickly we can train deep networks and how stable the training process is.\n",
    "\n",
    "## Goal: Reduce Variance to Speed Up and Stabilize Training\n",
    "\n",
    "To address the variance issue in SGD, two main techniques are commonly used:\n",
    "\n",
    "1. **Mini-Batch Gradient Descent**\n",
    "2. **Momentum**\n",
    "\n",
    "Each reduces variance in a different way — and they **stack** nicely when used together.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Mini-Batch Gradient Descent\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/392.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/393.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/394.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/395.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "Instead of computing the gradient on **a single training example**, we compute it on a **small batch** and average the gradients.\n",
    "\n",
    "This:\n",
    "- **Reduces variance** in the gradient estimate.\n",
    "- Makes the optimization path **smoother and more stable**.\n",
    "- **Speeds up convergence** while retaining much of the computational efficiency of SGD.\n",
    "\n",
    "### Batch Size\n",
    "\n",
    "How big should you make your batch size? The emprical answer: As big as possible, as big as you can fit on a single GPU. The limiting factor in this case, is the memory of the GPU you are using. This will not lead to batches that are too big or too slow to update, because the limiting factor on a GPU is the memory, not the actual computation.\n",
    "\n",
    "For larger batches, making the batch a power of two will aid the GPU, as GPUs are optimized for nicely rounded off sizes.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/396.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Trade-offs\n",
    "- A **larger batch** provides a better estimate of the true gradient (lower variance), but is **more expensive** to compute.\n",
    "- A **smaller batch** is cheaper but noisier (higher variance).\n",
    "\n",
    "#### Hyperparameter\n",
    "- The **batch size** becomes a critical tuning parameter.\n",
    "\n",
    "#### Special Cases:\n",
    "- Batch size = 1 → equivalent to vanilla SGD.\n",
    "- Batch size = dataset size → equivalent to full Gradient Descent.\n",
    "\n",
    "### Why It Works (Intuition)\n",
    "\n",
    "Averaging over multiple samples means the **random noise in individual gradients cancels out**, leaving a clearer signal of the \"true\" direction to descend in.\n",
    "\n",
    "Mathematically, if $\\hat{g}$ is the estimated gradient from a mini-batch and $g$ is the true full-dataset gradient:\n",
    "\n",
    "$\\text{Var}[\\hat{g}_{\\text{minibatch}}] \\leq \\text{Var}[\\hat{g}_{\\text{SGD}}]$\n",
    "\n",
    "### **Always Use Mini-Batches**\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Momentum\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/397.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "While mini-batches average across **samples**, momentum averages across **time** (steps).\n",
    "\n",
    "Momentum keeps track of an additional term, the average gradient. It will update this every single time a gradient is calculated using a running average.\n",
    "\n",
    "### Motivation:\n",
    "- Gradient directions from batch to batch can still be noisy.\n",
    "- Momentum helps **\"smooth out\" fluctuations** over time by incorporating the direction of previous gradients.\n",
    "\n",
    "### Mechanism\n",
    "\n",
    "In regular SGD, the model parameters are updated based **only on the current gradient**.  \n",
    "With **momentum**, we improve this by keeping a **running average** of the previous gradients — like adding \"inertia\" to our updates.\n",
    "\n",
    "Let:\n",
    "- $g_t$ be the current gradient at time step $t$\n",
    "- $v_t$ be the **velocity**, or the running average of past gradients\n",
    "\n",
    "The momentum update rule is:\n",
    "\n",
    "- $v_t = \\mu v_{t-1} + g_t$  → update the velocity by combining past and current gradients  \n",
    "- $\\theta_t = \\theta_{t-1} - \\epsilon v_t$  → update model parameters using the velocity\n",
    "\n",
    "Where:\n",
    "- $\\mu$ is the **momentum coefficient** (typically 0.9), which controls how much of the previous gradient history to keep\n",
    "- $\\epsilon$ is the **learning rate**\n",
    "\n",
    "Instead of following just the current gradient, momentum follows a **blend of the current and previous gradients**, resulting in faster and smoother convergence.\n",
    "\n",
    "### Benefits of Momentum\n",
    "\n",
    "- **Dampens oscillations** in directions with noisy gradients.\n",
    "- **Accelerates convergence** in consistently downhill directions.\n",
    "- Helps escape **shallow local minima** and avoid overshooting.\n",
    "\n",
    "### Hyperparameter:\n",
    "- **Momentum coefficient** ($\\mu$): typically set to **0.9** by default in practice.\n",
    "- PyTorch's `SGD` **does not set this automatically** — you must provide `momentum=0.9` manually.\n",
    "\n",
    "## Visualization: What Do These Look Like?\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/399.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/398.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "- **SGD**: erratic jumps, often zig-zagging and slow.\n",
    "- **Mini-Batch SGD**: smoother path, less jagged.\n",
    "- **Momentum**: smoother, faster convergence — often visually close to full gradient descent.\n",
    "\n",
    "Even mini-batches may produce spikes due to:\n",
    "- Loss evaluated on a hard sample\n",
    "- Taking a step in an imprecise direction\n",
    "\n",
    "## Momentum in Practice\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/3911.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Set it to 0.9 and leave it** — it works well in almost all practical cases.\n",
    "- If you forget to set it, **PyTorch will default to 0**, i.e., no momentum.\n",
    "- Other optimizers like Adam and RMSProp use momentum by default under the hood.\n",
    "\n",
    "---\n",
    "\n",
    "## Comparison Table\n",
    "\n",
    "|               | Mini-Batch SGD              | Momentum                          |\n",
    "|---------------|-----------------------------|-----------------------------------|\n",
    "| Averages over | Multiple samples             | Gradient history                  |\n",
    "| Reduces       | Sample-based variance        | Temporal variance (oscillations)  |\n",
    "| Hyperparam    | Batch size                   | Momentum factor $\\mu$             |\n",
    "| Cost          | Higher (more forward/backward passes) | Low (stores 1 additional gradient vector) |\n",
    "\n",
    "---\n",
    "\n",
    "## Final Summary\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/3912.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "- SGD works, but has **high variance**, especially with small batches or noisy data.\n",
    "- Use **mini-batches** to reduce sample-wise variance.\n",
    "- Use **momentum** to reduce noise over time and stabilize updates.\n",
    "- Both techniques are **simple**, **complementary**, and **widely used** in practice.\n",
    "- Always train your networks with **mini-batch SGD and momentum** — they're the default baseline for a reason.\n",
    "\n",
    "```python\n",
    "# Example in PyTorch\n",
    "torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
