{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43516bb5-11a7-4b0b-9664-8c112a3a7bb9",
   "metadata": {},
   "source": [
    "Some notes on `torch.rand` vs `torch.randn`:\n",
    "\n",
    "| Function    | Distribution      | Range     | Mean | Std Dev |\n",
    "|------------|------------------|----------|------|---------|\n",
    "| `torch.rand`  | Uniform          | [0, 1)   | 0.5  | ~0.29   |\n",
    "| `torch.randn` | Normal (Gaussian) | (-∞, ∞) | 0    | 1       |\n",
    "\n",
    "- Use **torch.rand** when you need uniform randomness (e.g., initializing weights between 0 and 1).\n",
    "- Use **torch.randn** when you need Gaussian-distributed randomness (e.g., adding noise to a model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ba8a3d2-4ec5-400f-b988-933ebc4ccf41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5213, 0.2838, 0.8541, 0.5054, 0.9969, 0.9367, 0.0811, 0.2498, 0.7966,\n",
       "        0.0846])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "y = torch.rand(10)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf09b9f8-b6b3-4cc8-bcc2-6b68b1e7d66a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9248, 0.0383, 0.5832, 0.2598, 0.7364, 0.4143, 0.5559, 0.6527, 0.9889,\n",
       "        0.3576], requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we add the requires_grad = True parameter, it adds this flag to the tensor\n",
    "x = torch.rand(10, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd41e93f-6a14-450d-a097-963a0e7afc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3919)\n",
      "tensor(0.3819, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Let's do some basic computation on each and see what happens\n",
    "print((y**2).mean())\n",
    "print((x**2).mean())\n",
    "\n",
    "# We can see that grad_fn is attached. This is attached to the result of any computation \n",
    "# on a tensor with requires_grad set to true. grad_fn is the function that pytorch uses\n",
    "# in the background to perform backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09960584-ac21-4c80-8bc9-ab603e51f121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3819, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# We can calculate gradients for anything that has grad_fn attached to it. Let's call \n",
    "# backward on that resultant and see.\n",
    "\n",
    "b = (x**2).mean()\n",
    "print(b)\n",
    "b.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8193b68c-d2b4-403c-a101-4303c6519395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1850, 0.0077, 0.1166, 0.0520, 0.1473, 0.0829, 0.1112, 0.1305, 0.1978,\n",
      "        0.0715])\n"
     ]
    }
   ],
   "source": [
    "# backward runs backpropagation, and populates grad, that gradient of the tensor.\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dcc966b4-1b51-40d9-967b-904fc3daa943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1850, 0.0077, 0.1166, 0.0520, 0.1473, 0.0829, 0.1112, 0.1305, 0.1978,\n",
      "        0.0715], grad_fn=<MulBackward0>)\n",
      "tensor([0.1850, 0.0077, 0.1166, 0.0520, 0.1473, 0.0829, 0.1112, 0.1305, 0.1978,\n",
      "        0.0715])\n"
     ]
    }
   ],
   "source": [
    "# We can prove this by undoing our computation and comparing to the \n",
    "# original x:\n",
    "print(x/10 * 2)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c682f9-cfdf-459a-ab32-cf496e4dcced",
   "metadata": {},
   "source": [
    "Once we set the `requires_grad` parameter to true, pytorch automatically\n",
    "builds the computation graph for that tensor. This allows us to call `backward` on a\n",
    "scalar when we are ready to compute the gradient.\n",
    "\n",
    "We cannot call backward twice on the same scalar. If we call backward multiple times, it will begin to sum the gradients. \n",
    "\n",
    "It is important to note that building the computation graph using `requires_grad` does \n",
    "cost a decent chunk of memory, but when you call `backward` it will collapse the tree and\n",
    "reduce the memory overhead significantly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
