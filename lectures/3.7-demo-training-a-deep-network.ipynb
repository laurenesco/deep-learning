{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64a7acc8-8d4d-428a-b6eb-bc1afc699add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Training a Deep Network on Flowers102 with PyTorch\n",
    "# --------------------------------------------------\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Use GPU if available, otherwise fallback to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c583520a-9c66-4a7f-b4c0-be727835647c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Flowers102\n",
      "    Number of datapoints: 1020\n",
      "    Root location: ./flowers\n",
      "    split=train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(128, 128), interpolation=bilinear, max_size=None, antialias=True)\n",
      "               ToTensor()\n",
      "           )\n",
      "Dataset Flowers102\n",
      "    Number of datapoints: 6149\n",
      "    Root location: ./flowers\n",
      "    split=test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(128, 128), interpolation=bilinear, max_size=None, antialias=True)\n",
      "               ToTensor()\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Step 1: Data Preparation\n",
    "# --------------------------\n",
    "\n",
    "# Define transformations for input images:\n",
    "# 1. Resize all images to 128x128 — because the original dataset has images of varying sizes.\n",
    "# 2. Convert PIL image to PyTorch tensor for model compatibility.\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load the Flowers102 dataset\n",
    "# The dataset will return a tuple (image_tensor, label) where label is the integer class index.\n",
    "train_dataset = datasets.Flowers102(\"./flowers\", split='train', download=True, transform=transform)\n",
    "test_dataset = datasets.Flowers102(\"./flowers\", split='test', download=True, transform=transform)\n",
    "\n",
    "# Dataset objects implement:\n",
    "# - __len__ to return the number of samples\n",
    "# - __getitem__ to return a single (image, label) pair with transforms applied\n",
    "# Access samples via indexing: e.g., `train_dataset[0]`\n",
    "\n",
    "# Take a peek\n",
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7be6f03-9525-4502-9ff1-7ebb09ee5382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1db6eb958d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Step 2: DataLoader\n",
    "# --------------------------\n",
    "\n",
    "# Wrap datasets in DataLoaders to:\n",
    "# - Automatically form mini-batches\n",
    "# - Optionally shuffle data (important for SGD)\n",
    "# - Use multiple worker processes for speed\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,      # 64 is a typical batch size that balances speed and stability\n",
    "    shuffle=True,       # Crucial for SGD convergence\n",
    "    num_workers=2       # Use multiple subprocesses to load data faster (especially helpful with large image datasets)\n",
    ")\n",
    "\n",
    "# Important DataLoader options explained:\n",
    "# - shuffle: Avoids ordering artifacts by reshuffling data each epoch.\n",
    "# - num_workers: Parallelizes disk access/loading; >0 can significantly improve I/O performance.\n",
    "# - drop_last: If dataset size isn't divisible by batch size, drop the last small batch (optional).\n",
    "# - pin_memory: For GPU training, speeds up host-to-device transfer.\n",
    "\n",
    "# Take a peek\n",
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe32fc16-8045-4aea-9086-ea5cd92a4493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Step 3: Model Definition\n",
    "# --------------------------\n",
    "\n",
    "# Define a Multi-Layer Perceptron (MLP) with configurable hidden layers\n",
    "# Input size is flattened 128x128 RGB image → 128*128*3 features\n",
    "\n",
    "# Note about layer sizes: layer_sizes defines the number and size of hidden layers in the model.\n",
    "# For example, [512, 512, 512] will create three hidden layers, each with 512 units.\n",
    "# \n",
    "# This design makes the model size configurable — you can easily experiment with\n",
    "# deeper or shallower networks by passing different layer sizes when instantiating the model.\n",
    "# \n",
    "# Example usage:\n",
    "# - MLP([256, 128]) creates a smaller 2-layer model\n",
    "# - MLP([1024, 512, 256, 128]) creates a deeper and wider model\n",
    "#\n",
    "# The last layer (output layer) is always fixed to have 102 outputs (number of classes),\n",
    "# so you only control the *intermediate* layers via this list.\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, layer_sizes=[512, 512, 512]):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        input_size = 128 * 128 * 3  # Image dimensions flattened\n",
    "        layers.append(nn.Flatten())  # Turn each image into a vector\n",
    "\n",
    "        # Add fully-connected layers with ReLU activations\n",
    "        for size in layer_sizes:\n",
    "            layers.append(nn.Linear(input_size, size))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_size = size  # Update input size for next layer\n",
    "\n",
    "        # Final layer maps to 102 flower classes (no softmax needed with CrossEntropyLoss)\n",
    "        layers.append(nn.Linear(input_size, 102))\n",
    "        \n",
    "        # Create a network where the output of one layer is fed in as the input of next layer \n",
    "        # until nothing is left\n",
    "        self.model = nn.Sequential(*layers) \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Instantiate model and move to correct device\n",
    "model = MLP().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9b35e26-9df0-4207-a32d-4223ac290070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Step 4: Loss and Optimizer\n",
    "# --------------------------\n",
    "\n",
    "# Use CrossEntropyLoss — combines LogSoftmax + NLLLoss internally\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Use SGD with momentum; Adam can also be tried in later experiments\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40a72e45-c83d-49a4-9145-f544b1ed488d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss Value: 4.635961055755615, Total Loss: 74.0531\n",
      "Epoch 2, Loss Value: 4.586237907409668, Total Loss: 73.8435\n",
      "Epoch 3, Loss Value: 4.593423366546631, Total Loss: 73.3589\n",
      "Epoch 4, Loss Value: 4.436051368713379, Total Loss: 72.1184\n",
      "Epoch 5, Loss Value: 4.217155456542969, Total Loss: 69.3830\n",
      "Epoch 6, Loss Value: 4.009844779968262, Total Loss: 64.8913\n",
      "Epoch 7, Loss Value: 3.9751429557800293, Total Loss: 62.5397\n",
      "Epoch 8, Loss Value: 3.7055091857910156, Total Loss: 59.6945\n",
      "Epoch 9, Loss Value: 3.7261412143707275, Total Loss: 56.3934\n",
      "Epoch 10, Loss Value: 3.655364513397217, Total Loss: 54.0248\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Step 5: Training Loop\n",
    "# --------------------------\n",
    "\n",
    "# One \"epoch\" = one full pass through the training dataset\n",
    "# Typically, we train for many epochs (e.g., 10s–100s)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        # Each batch is a tuple (images, labels)\n",
    "        images, labels = batch\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        preds = model(images)\n",
    "\n",
    "        # Compute the loss (how wrong the model is)\n",
    "        loss = loss_fn(preds, labels)\n",
    "\n",
    "        # Backward pass: compute gradients\n",
    "        optimizer.zero_grad()      # Clear previous gradients\n",
    "        loss.backward()            # Compute gradients\n",
    "        optimizer.step()           # Update model weights\n",
    "\n",
    "        # A note on losses: \n",
    "        # loss.item() gives the loss value for the **most recent batch**.\n",
    "        # total_loss is the **sum of all batch losses** across the entire epoch.\n",
    "        # Printing both helps you see batch-level vs. epoch-level progress:\n",
    "        # - loss.item() fluctuates (some batches are harder)\n",
    "        # - total_loss (or its average) should go down steadily across epochs\n",
    "        total_loss += loss.item()  # Accumulate batch loss\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss Value: {loss.item()}, Total Loss: {total_loss:.4f}\")\n",
    "\n",
    "# --------------------------\n",
    "# Notes & Performance Tips\n",
    "# --------------------------\n",
    "\n",
    "# - Each batch may vary in difficulty → causes small fluctuations in loss.\n",
    "# - Loss should decrease over time, but not always in a straight line.\n",
    "# - Low learning rate = slow convergence; high learning rate = risk of divergence or oscillation.\n",
    "# - If loss increases → try lowering the learning rate.\n",
    "# - If model isn’t learning, try:\n",
    "#   - Using fewer or simpler layers (e.g., linear model without hidden layers)\n",
    "#   - Increasing learning rate gradually\n",
    "#   - Double-checking image sizes and transformations\n",
    "# - Slowdowns may be due to:\n",
    "#   - Data loading from disk at each batch\n",
    "#   - CPU-only training\n",
    "#   - Solutions: use GPU, increase num_workers, cache data in memory\n",
    "\n",
    "# --------------------------\n",
    "# Summary\n",
    "# --------------------------\n",
    "\n",
    "# A full PyTorch training pipeline includes:\n",
    "# 1. Data loading and preprocessing with transforms\n",
    "# 2. Dataset and DataLoader setup\n",
    "# 3. Model definition with `nn.Module`\n",
    "# 4. Loss function and optimizer selection\n",
    "# 5. Epoch + batch-level training loop:\n",
    "#    - forward → loss → backward → step\n",
    "# 6. Optional: logging/printing loss for monitoring\n",
    "\n",
    "# Training deep networks really boils down to:\n",
    "# → A double for loop with a handful of function calls\n",
    "\n",
    "# This foundational structure will stay the same even when we:\n",
    "# - Switch to CNNs or transformers\n",
    "# - Use fancier optimizers or loss functions\n",
    "# - Add validation and testing loops\n",
    "\n",
    "\n",
    "# Tip: use help() to learn more about any function, e.g. \"help(torch.utils.data.DataLoader())\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
