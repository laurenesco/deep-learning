{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de8eb31a-adce-4aeb-ab8e-ab9b16bd29ec",
   "metadata": {},
   "source": [
    "# Activation Functions in Deep Networks\n",
    "\n",
    "## What Is an Activation Function?\n",
    "- An activation function is the **non-linear transformation** placed between linear layers.\n",
    "- Deep networks = alternating linear layers and non-linear activations.\n",
    "- This structure gives deep networks their **expressive power**.\n",
    "\n",
    "There are many different functions people sandwich between linear layers, including:\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/zoo.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "## ReLU (Rectified Linear Unit)\n",
    "- Defined as:\n",
    "  $$\n",
    "  \\text{ReLU}(x) = \\max(0, x)\n",
    "  $$\n",
    "- Behavior:\n",
    "  - Linear for $( x > 0 $)\n",
    "  - Zero for $( x < 0 $)\n",
    "  - Gradient is:\n",
    "    - 1 for $( x > 0 $)\n",
    "    - 0 for $( x < 0 $)\n",
    "    - Undefined at $( x = 0 $), but handled arbitrarily in practice\n",
    "- Pros:\n",
    "  - Very fast and simple\n",
    "  - Easy to compute\n",
    "- Cons:\n",
    "  - **\"Dead ReLU\" problem**: If the activation becomes negative and stays negative, it will never recover — no gradient flows back\n",
    "- Mitigation:\n",
    "  - Use careful **initialization**\n",
    "  - Try smaller **learning rates**\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/relu2.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "## Leaky ReLU\n",
    "- Modification of ReLU:\n",
    "  $$\n",
    "  \\text{LeakyReLU}(x) = \\max(\\alpha x, x)\n",
    "  $$\n",
    "- Adds a small slope $(\\alpha > 0$) on the negative side.\n",
    "- Pros:\n",
    "  - Solves the \"dead ReLU\" problem by allowing a small gradient when $( x < 0 $)\n",
    "- Cons:\n",
    "  - Requires tuning of $\\alpha$)\n",
    "  - Can allow unwanted signal \"leakage\" through negative values\n",
    "- Variants:\n",
    "  - **Parametric ReLU (PReLU)**: learns $\\alpha$ during training\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/lrelu.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "## ELU (Exponential Linear Unit)\n",
    "- Defined as:\n",
    "  $$\n",
    "  \\text{ELU}(x) =\n",
    "  \\begin{cases}\n",
    "  x & \\text{if } x > 0 \\\\\n",
    "  \\alpha(\\exp(x) - 1) & \\text{if } x \\leq 0\n",
    "  \\end{cases}\n",
    "  $$\n",
    "- Pros:\n",
    "  - Smooth transition and non-zero gradients even for $( x < 0 $)\n",
    "  - Helps maintain mean activations near zero\n",
    "- Cons:\n",
    "  - Requires computation of $\\exp(x)$, which is computationally expensive\n",
    "  - Requires tuning of $\\alpha$\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/elu.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "## GELU (Gaussian Error Linear Unit)\n",
    "- Combines:\n",
    "  - Linear function $ x $\n",
    "  - With a Gaussian error function\n",
    "- Behavior:\n",
    "  - Approximately linear for $ x > 0 $\n",
    "  - Dampens signal for $ x < 0 $ by multiplying with a value near zero\n",
    "- Unique Feature:\n",
    "  - Has a small **dip below zero**, meaning two different inputs may have the same output,\n",
    "    this was a concern but deep networks have shown to perform better with the dips.\n",
    "- Pros:\n",
    "  - Empirically shown to perform well in modern deep networks\n",
    "  - Gradient is non-zero everywhere\n",
    "- Cons:\n",
    "  - Computationally expensive (requires Gaussian PDF computation)\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/gelu.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "## Sigmoid and Tanh (Legacy Functions)\n",
    "- **Sigmoid**:\n",
    "  $$\n",
    "  \\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "  $$\n",
    "- **Tanh**:\n",
    "  $$\n",
    "  \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "  $$\n",
    "- These are rarely used today because:\n",
    "  - They **saturate** on both ends (output approaches constant values)\n",
    "  - Gradients become very small in saturation zones → hard to train\n",
    "- Only effective if activations are kept in narrow central range\n",
    "\n",
    "## Summary: Which Activation Should You Use?\n",
    "\n",
    "| Activation | Pros | Cons | Use Case |\n",
    "|------------|------|------|----------|\n",
    "| **ReLU** | Fast, simple | Can \"die\" (zero gradients) | Default starting point |\n",
    "| **Leaky ReLU** | Prevents dead neurons | Needs tuning | Debugging ReLU issues |\n",
    "| **PReLU** | Learns alpha | Rarely used now | Specialized networks |\n",
    "| **ELU** | Smooth, always has gradient | Slower | Useful if mean-zero activations help |\n",
    "| **GELU** | Best empirical results | Slow, complex | State-of-the-art networks |\n",
    "| **Sigmoid/Tanh** | Historically common | Saturation, vanishing gradients | Avoid unless necessary |\n",
    "\n",
    "## Practical Tips\n",
    "- **Start with ReLU**.\n",
    "- If ReLU units are dying (many outputs stuck at 0):\n",
    "  - Try **Leaky ReLU** or **PReLU**.\n",
    "- Avoid **Sigmoid** and **Tanh** unless you have a specific reason.\n",
    "- For **high-performance models** (e.g., Transformers, BERT), consider **GELU**.\n",
    "- PyTorch default initializations are usually good.\n",
    "- Use a **small learning rate** to prevent blowing up activations.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
