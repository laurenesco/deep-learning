{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ae7c73c-1cb0-4092-9611-0cdfc868941e",
   "metadata": {},
   "source": [
    "# Lecture Notes: Summary and Guidelines for Training Basic Deep Networks\n",
    "\n",
    "## **Step 1: Determine the Type of Task**\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/311.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "Ask yourself: **What kind of output is your model predicting?**\n",
    "\n",
    "- Continuous values → You’re doing **regression**\n",
    "- Discrete labels → You’re doing **classification**\n",
    "\n",
    "## **For Regression Tasks**\n",
    "\n",
    "If your model outputs continuous numbers:\n",
    "\n",
    "- Use **L1 loss** (Mean Absolute Error) or  \n",
    "- Use **L2 loss** (Mean Squared Error), also called **MSELoss**\n",
    "\n",
    "In practice, L1 and L2 often work similarly. Start with MSELoss unless you have a specific reason to prefer L1.\n",
    "\n",
    "## **For Classification Tasks**\n",
    "\n",
    "If your model predicts classes, ask:\n",
    "\n",
    "**How many classes are there?**\n",
    "\n",
    "### Case 1: Binary Classification (2 classes)\n",
    "\n",
    "- Use: `BCEWithLogitsLoss` (Binary Cross Entropy with logits)\n",
    "- Output: One scalar per sample\n",
    "\n",
    "### Case 2: Multi-Class Classification (more than 2 classes)\n",
    "\n",
    "- Use: `CrossEntropyLoss`\n",
    "- Output: One logit per class, shape `[batch_size, num_classes]`\n",
    "\n",
    "Tip: You can use `CrossEntropyLoss` with 2 classes if you output 2 values instead of 1.\n",
    "\n",
    "## **Multi-Label Classification (Tagging)**\n",
    "\n",
    "If you’re predicting **multiple binary labels per sample**:\n",
    "\n",
    "- Use: `BCEWithLogitsLoss` with multiple outputs\n",
    "- Avoid: `CrossEntropyLoss`, because it assumes mutual exclusivity\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 2: Choose an Activation Function**\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/3111.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Default**: `ReLU` — it’s fast, simple, and works well in most cases\n",
    "- Use `LeakyReLU` or `PReLU` if you encounter “dead” ReLU units\n",
    "\n",
    "If not using a custom model, stick with whatever activation a known architecture already uses unless you have a reason to change it.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 3: Pick an Optimizer**\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/3112.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "Use **SGD** (Stochastic Gradient Descent) with:\n",
    "\n",
    "- The largest batch size that fits in your GPU memory\n",
    "- `momentum=0.9`\n",
    "\n",
    "Example:\n",
    "\n",
    "`torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)`\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 4: Tune the Learning Rate**\n",
    "\n",
    "Learning rate (`lr`) is **the most important hyperparameter** to tune.\n",
    "\n",
    "### Strategy:\n",
    "\n",
    "1. Start small (e.g., `0.001`)\n",
    "2. Increase until the loss starts spiking → too high\n",
    "3. If the model diverges (e.g., `loss = NaN`) → way too high\n",
    "4. Back off slightly and use the largest stable value\n",
    "\n",
    "Small learning rates = slow learning  \n",
    "Large learning rates = unstable, can “explode” the model\n",
    "\n",
    "---\n",
    "\n",
    "## Final Notes\n",
    "\n",
    "You now know how to:\n",
    "\n",
    "- Choose a loss function based on your task type\n",
    "- Use `ReLU` as a default activation\n",
    "- Set up SGD with momentum\n",
    "- Tune the learning rate\n",
    "\n",
    "These form the foundation for training any deep neural network.\n",
    "\n",
    "---\n",
    "\n",
    "## What’s Next\n",
    "\n",
    "In the rest of the course, you’ll learn:\n",
    "\n",
    "- Architectural innovations\n",
    "- Tricks for deeper networks\n",
    "- Data-specific design ideas\n",
    "- A “bag of tricks” to help you train better models\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
